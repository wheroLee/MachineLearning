{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bittf21condacacb7fb5d6184e80be7a052c9d395d5b",
   "display_name": "Python 3.7.6 64-bit ('tf21': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The study for HMC BIW PROJECT  \n",
    "\n",
    "## M/L is used to estimate the value with parameters.  \n",
    "## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# tensorflow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "# other utiles\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the tensorflow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training dataset\n",
    "dataset_org = pd.read_csv(r'F:\\CON_2019_HMC_PGD-MINESET\\Second_model\\Case2\\whe_ro_to_send\\New_Testing_parameters_high_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. M/L model for Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an instance for dataset\n",
    "dataset = dataset_org\n",
    "# extract the columns from the dataset\n",
    "column_names = dataset.columns\n",
    "# dataset의 column_names 업데이트\n",
    "# 1. 행번호(row number)로 선택하는 방법 (.iloc)\n",
    "# 2. label이나 조건표현으로 선택하는 방법 (.loc)\n",
    "#dataset = dataset.loc[:,column_names]\n",
    "# check dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Pelvic culumn from the dataset\n",
    "dataset.pop(\"Pelvic\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training & test dataset\n",
    "# for training, 60% is used for training.\n",
    "# sampling is by random\n",
    "\n",
    "# DataFrame.sample(self: ~ FrameOrSeries, n=None, frac=None, replace=False, weights=None, random_state=None, axis=None)\n",
    "# Parameters\n",
    "# n; int, optional\n",
    "#    Number of items from axis to return. Cannot be used with frac. Default = 1 if frac = None.\n",
    "# frac; float, optional\n",
    "#    Fraction of axis items to return. Cannot be used with n.\n",
    "# replace; bool, default False\n",
    "#    Allow or disallow sampling of the same row more than once.\n",
    "# weights; str or ndarray-like, optional\n",
    "#   Default ‘None’ results in equal probability weighting. If passed a Series, will align with target object on index. Index values in weights not found in sampled object will be ignored and index values in sampled object not in weights will be assigned weights of zero. If called on a DataFrame, will accept the name of a column when axis = 0. Unless weights are a Series, weights must be same length as axis being sampled. If weights do not sum to 1, they will be normalized to sum to 1. Missing values in the weights column will be treated as zero. Infinite values not allowed.\n",
    "# random_state; int or numpy.random.RandomState, optional\n",
    "#   Seed for the random number generator (if int), or numpy RandomState object.\n",
    "# axis; {0 or ‘index’, 1 or ‘columns’, None}, default None\n",
    "#   Axis to sample. Accepts axis number or name. Default is stat axis for given data type (0 for Series and DataFrames). \n",
    "train_dataset = dataset.sample(frac=0.7)\n",
    "\n",
    "\n",
    "# DataFrame.drop(self, labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n",
    "test_dataset = dataset.drop(train_dataset.index)\t\n",
    "# test_dataset = dataset\n",
    "\n",
    "train_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data 산포 분석\n",
    "\n",
    "# seaborn.pairplot(data, hue=None, hue_order=None, palette=None, vars=None, x_vars=None, y_vars=None, kind='scatter', diag_kind='auto', markers=None, height=2.5, aspect=1, corner=False, dropna=True, plot_kws=None, diag_kws=None, grid_kws=None, size=None)\n",
    "# Parameters\n",
    "# dataDataFrame\n",
    "# Tidy (long-form) dataframe where each column is a variable and each row is an observation.\n",
    "\n",
    "# huestring (variable name), optional\n",
    "# Variable in data to map plot aspects to different colors.\n",
    "\n",
    "# hue_orderlist of strings\n",
    "# Order for the levels of the hue variable in the palette\n",
    "\n",
    "# palettedict or seaborn color palette\n",
    "# Set of colors for mapping the hue variable. If a dict, keys should be values in the hue variable.\n",
    "\n",
    "# varslist of variable names, optional\n",
    "# Variables within data to use, otherwise use every column with a numeric datatype.\n",
    "\n",
    "# {x, y}_varslists of variable names, optional\n",
    "# Variables within data to use separately for the rows and columns of the figure; i.e. to make a non-square plot.\n",
    "\n",
    "# kind{‘scatter’, ‘reg’}, optional\n",
    "# Kind of plot for the non-identity relationships.\n",
    "\n",
    "# diag_kind{‘auto’, ‘hist’, ‘kde’, None}, optional\n",
    "# Kind of plot for the diagonal subplots. The default depends on whether \"hue\" is used or not.\n",
    "\n",
    "# markerssingle matplotlib marker code or list, optional\n",
    "# Either the marker to use for all datapoints or a list of markers with a length the same as the number of levels in the hue variable so that differently colored points will also have different scatterplot markers.\n",
    "\n",
    "# heightscalar, optional\n",
    "# Height (in inches) of each facet.\n",
    "\n",
    "# aspectscalar, optional\n",
    "# Aspect * height gives the width (in inches) of each facet.\n",
    "\n",
    "# cornerbool, optional\n",
    "# If True, don’t add axes to the upper (off-diagonal) triangle of the grid, making this a “corner” plot.\n",
    "\n",
    "# dropnaboolean, optional\n",
    "# Drop missing values from the data before plotting.\n",
    "\n",
    "# {plot, diag, grid}_kwsdicts, optional\n",
    "# Dictionaries of keyword arguments. plot_kws are passed to the bivariate plotting function, diag_kws are passed to the univariate plotting function, and grid_kws are passed to the PairGrid constructor.\n",
    "\n",
    "# fig = sns.pairplot(train_dataset[[\"HLAYER01\", \"HLAYER02\", \"HLAYER03\",\"HLAYER04\",\"HLAYER05\",\"Area\"]], diag_kind=\"kde\")\t\n",
    "# fig.savefig('foo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the training data status\n",
    "\n",
    "# DataFrame.describe(self: ~ FrameOrSeries, percentiles=None, include=None, exclude=None) \n",
    "# make an instance for dataframe as \"train_stats\"\n",
    "train_stats = train_dataset.describe()\n",
    "# delete \"Area\" column from \"train_stats\"\n",
    "train_stats.pop(\"Area\")\n",
    "\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make instances for dataset without the QoI\n",
    "train_labels = train_dataset.pop('Area')\n",
    "test_labels = test_dataset.pop('Area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of each dataset instance\n",
    "# Question???\n",
    "# Although all parameters are material stiffness, normalization have to proceed? \n",
    "def norm(x):\n",
    "  return (x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "normed_train_data = norm(train_dataset)\n",
    "normed_test_data  = norm(test_dataset)\n",
    "\n",
    "# normed_train_data = train_dataset\n",
    "# normed_test_data  = test_dataset\n",
    "normed_train_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M/L model building\n",
    "# Regularizer는 최적화 과정 중에 각 층별 파라미터 또는 출력값에 대하여 페널티를 적용할 수 있게 해줍니다. 이러한 페널티는 네트워크가 최적화 하려는 손실 함수의 일부로 포함됩니다.\n",
    "# Dense = 56(input) X 1(output) = 56\n",
    "\n",
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "      layers.Dense(56, activation='relu', \n",
    "                   kernel_regularizer=keras.regularizers.l2(0.01), \n",
    "                   input_shape=[len(train_dataset.keys())]),\n",
    "    #   layers.Dropout(0.5),\n",
    "      layers.Dense(56, activation='relu', \n",
    "                   kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "                   ), \n",
    "    #   layers.Dropout(0.5),\n",
    "      layers.Dense(1)\n",
    "      ])\n",
    "    #optimizer = tf.keras.optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)\n",
    "    #optimizer = tf.keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "    #optimizer = tf.keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    #optimizer = tf.keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #optimizer = tf.keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    #optimizer = tf.keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    \n",
    "    model.compile(\n",
    "      loss='mae',\n",
    "      optimizer=optimizer,\n",
    "      # 'mae', 'mse', 'mape',\n",
    "      metrics=['mae'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the model instance \n",
    "model_Area = build_model()\n",
    "model_Area.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the M/L model with small cases\n",
    "# example_batch  = normed_train_data[:10]\n",
    "example_batch  = normed_train_data[:5]\n",
    "example_result = model_Area.predict(example_batch)\n",
    "example_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "\n",
    "# model.fit(data, labels, epochs=10, batch_size=32, validation_data=(val_data, val_labels))\n",
    "# tf.keras.Model.fit에는 세 개의 중요한 매개변수가 있습니다:\n",
    "# epochs: 훈련은 에포크(epoch)로 구성됩니다. 한 에포크는 전체 입력 데이터를 한번 순회하는 것입니다(작은 배치로 나누어 수행됩니다).\n",
    "# batch_size: 넘파이 데이터를 전달하면 모델은 데이터를 작은 배치로 나누고 훈련 과정에서 이 배치를 순회합니다. 이 정수 값은 배치의 크기를 지정합니다. 전체 샘플 개수가 배치 크기로 나누어 떨어지지 않으면 마지막 배치의 크기는 더 작을 수 있습니다.\n",
    "# validation_data: 모델의 프로토타입(prototype)을 만들 때는 검증 데이터(validation data)에서 간편하게 성능을 모니터링해야 합니다. 입력과 레이블(label)의 튜플을 이 매개변수로 전달하면 에포크가 끝날 때마다 추론 모드(inference mode)에서 전달된 데이터의 손실과 측정 지표를 출력합니다.\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "history = model_Area.fit(\n",
    "                          normed_train_data, train_labels,\n",
    "                          epochs=EPOCHS, validation_split = 0.2, verbose=0,\n",
    "                          callbacks=[tfdocs.modeling.EpochDots()]\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_mae_score, test_mae_score = model_Area.evaluate(  normed_test_data, test_labels  )\n",
    "test_mae_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반화, 과대적합, 과소적합 확인 \n",
    "plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)\n",
    "plotter.plot({'Basic': history}, metric = \"mae\")\n",
    "plt.ylim([0, 600])\n",
    "plt.ylabel('MAE [Area]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# early stop by val_mae\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='mae', patience=10)\n",
    "\n",
    "early_history = model_Area.fit(normed_train_data, train_labels, \n",
    "                    epochs=EPOCHS, validation_split = 0.2, verbose=0, \n",
    "                    callbacks=[early_stop, tfdocs.modeling.EpochDots()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot({'Early Stopping': early_history}, metric = \"mae\")\n",
    "plt.ylim([0, 600])\n",
    "plt.ylabel('MAE [Area]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model evaluate \n",
    "loss, mae = model_Area.evaluate(normed_test_data, test_labels, verbose=2)\n",
    "print(\"Testing set Mean Abs Error: Area {:5.2f} [mm2]\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluate by curve\n",
    "test_predictions = model_Area.predict(normed_test_data).flatten()\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('True Values [Area]')\n",
    "plt.ylabel('Predictions [Area]')\n",
    "lims = [0, 4000]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. M/L model for Pelvic displacement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training dataset\n",
    "dataset_org = pd.read_csv(r'F:\\CON_2019_HMC_PGD-MINESET\\Second_model\\Case2\\whe_ro_to_send\\New_Testing_parameters_high_filtered.csv')\n",
    "# make an instance for dataset\n",
    "dataset = dataset_org\n",
    "# extract the columns from the dataset\n",
    "column_names = dataset.columns\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Pelvic culumn from the dataset\n",
    "dataset.pop(\"Area\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training & test dataset\n",
    "# for training, 70% is used for training.\n",
    "# sampling is by random\n",
    "train_dataset = dataset.sample(frac=0.7)\n",
    "\n",
    "# DataFrame.drop(self, labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n",
    "test_dataset = dataset.drop(train_dataset.index)\t\n",
    "# test_dataset = dataset\n",
    "\n",
    "train_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 산포 분석\n",
    "\n",
    "# fig = sns.pairplot(train_dataset[[\"HLAYER01\", \"HLAYER02\", \"HLAYER03\",\"HLAYER04\",\"HLAYER05\",\"Area\"]], diag_kind=\"kde\")\t\n",
    "# fig.savefig('foo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the training data status\n",
    "\n",
    "# DataFrame.describe(self: ~ FrameOrSeries, percentiles=None, include=None, exclude=None) \n",
    "# make an instance for dataframe as \"train_stats\"\n",
    "train_stats = train_dataset.describe()\n",
    "# delete \"Area\" column from \"train_stats\"\n",
    "train_stats.pop(\"Pelvic\")\n",
    "\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make instances for dataset without the QoI\n",
    "train_labels = train_dataset.pop('Pelvic')\n",
    "test_labels = test_dataset.pop('Pelvic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalization of each dataset instance\n",
    "# # Question???\n",
    "# # Although all parameters are material stiffness, normalization have to proceed? \n",
    "# def norm(x):\n",
    "#   return (x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "# normed_train_data = norm(train_dataset)\n",
    "# normed_test_data  = norm(test_dataset)\n",
    "\n",
    "# normed_train_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the model instance \n",
    "model_Pelvic = build_model()\n",
    "model_Pelvic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the M/L model with small cases\n",
    "# example_batch  = normed_train_data[:10]\n",
    "example_batch  = normed_train_data[:5]\n",
    "example_result = model_Pelvic.predict(example_batch)\n",
    "example_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5000\n",
    "\n",
    "history = model_Pelvic.fit(\n",
    "                          normed_train_data, train_labels,\n",
    "                          epochs=EPOCHS, validation_split = 0.2, verbose=0,\n",
    "                          callbacks=[tfdocs.modeling.EpochDots()]\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_mae_score, test_mae_score = model_Pelvic.evaluate(  normed_test_data, test_labels  )\n",
    "test_mae_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반화, 과대적합, 과소적합 확인 \n",
    "plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)\n",
    "plotter.plot({'Basic': history}, metric = \"mae\")\n",
    "plt.ylim([0, 10])\n",
    "plt.ylabel('MAE [Pelvic displacement]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# early stop by val_mae\n",
    "\n",
    "early_stop_pelvic = keras.callbacks.EarlyStopping(monitor='mae', patience=10)\n",
    "\n",
    "early_history_pelvic = model_Pelvic.fit(normed_train_data, train_labels, \n",
    "                    epochs=EPOCHS, validation_split = 0.2, verbose=0, \n",
    "                    callbacks=[early_stop, tfdocs.modeling.EpochDots()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot({'Early Stopping': early_history_pelvic}, metric = \"mae\")\n",
    "plt.ylim([0, 10])\n",
    "plt.ylabel('MAE [Pelvic displacement]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model evaluate \n",
    "loss, mae = model_Pelvic.evaluate(normed_test_data, test_labels, verbose=2)\n",
    "print(\"Testing set Mean Abs Error: Pelvic disp {:5.2f} [mm]\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluate by curve\n",
    "test_predictions = model_Pelvic.predict(normed_test_data).flatten()\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('True Values [Disp]')\n",
    "plt.ylabel('Predictions [Disp]')\n",
    "lims = [60, 90]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimization by Generic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from scipy import optimize\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Curve points\n",
    "point = 2480\n",
    "# Number of variables \n",
    "var = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_func\n",
    "# Mean square error between test and prediction\n",
    "\n",
    "def predict_func(column_names):\n",
    "\n",
    "    min_Area   = 56.5625\n",
    "    max_Area   = 14282.91\n",
    "    min_Pelvic = 66.54655\n",
    "    max_Pelvic = 101.0649\n",
    "\n",
    "    for i in range( 0, len(column_names) ):\n",
    "        if column_names[i] <= 515 :\n",
    "            column_names[i] = 440\n",
    "        elif 515 < column_names[i] <= 685:\n",
    "            column_names[i] = 590\n",
    "        elif 685 < column_names[i] <= 880:\n",
    "            column_names[i] = 780\n",
    "        elif 880 < column_names[i] <= 1080:\n",
    "            column_names[i] = 980\n",
    "        elif 1080 < column_names[i] <= 1290:\n",
    "            column_names[i] = 1180\n",
    "        elif 1290 < column_names[i] :\n",
    "            column_names[i] = 1400\n",
    "        \n",
    "\n",
    "    PARTNO01 = column_names[0]\n",
    "    PARTNO02 = column_names[1]\n",
    "    PARTNO03 = column_names[2]\n",
    "    PARTNO04 = column_names[3]\n",
    "    PARTNO05 = column_names[4]\n",
    "    PARTNO06 = column_names[5]\n",
    "    PARTNO07 = column_names[6]\n",
    "    PARTNO08 = column_names[7]\n",
    "    PARTNO09 = column_names[8]\n",
    "    PARTNO10 = column_names[9]\n",
    "\n",
    "    PARTNO11 = column_names[10]\n",
    "    PARTNO12 = column_names[11]\n",
    "    PARTNO13 = column_names[12]\n",
    "    PARTNO14 = column_names[13]\n",
    "    PARTNO15 = column_names[14]\n",
    "    PARTNO16 = column_names[15]\n",
    "    PARTNO17 = column_names[16]\n",
    "    PARTNO18 = column_names[17]\n",
    "    PARTNO19 = column_names[18]\n",
    "    PARTNO20 = column_names[19]\n",
    "\n",
    "    PARTNO21 = column_names[20]\n",
    "    PARTNO22 = column_names[21]\n",
    "    PARTNO23 = column_names[22]\n",
    "    PARTNO24 = column_names[23]\n",
    "    PARTNO25 = column_names[24]\n",
    "    PARTNO26 = column_names[25]\n",
    "    PARTNO27 = column_names[26]\n",
    "    PARTNO28 = column_names[27]\n",
    "    PARTNO29 = column_names[28]\n",
    "    PARTNO30 = column_names[29]\n",
    "\n",
    "    PARTNO31 = column_names[30]\n",
    "    PARTNO32 = column_names[31]\n",
    "    PARTNO33 = column_names[32]\n",
    "    PARTNO34 = column_names[33]\n",
    "    PARTNO35 = column_names[34]\n",
    "    PARTNO36 = column_names[35]\n",
    "    PARTNO37 = column_names[36]\n",
    "    PARTNO38 = column_names[37]\n",
    "    PARTNO39 = column_names[38]\n",
    "    PARTNO40 = column_names[39]\n",
    "\n",
    "    PARTNO41 = column_names[40]\n",
    "    PARTNO42 = column_names[41]\n",
    "    PARTNO43 = column_names[42]\n",
    "    PARTNO44 = column_names[43]\n",
    "    PARTNO45 = column_names[44]\n",
    "\n",
    "    PARTNO46 = column_names[45]\n",
    "    PARTNO47 = column_names[46]\n",
    "    PARTNO48 = column_names[47]\n",
    "    PARTNO49 = column_names[48]\n",
    "    PARTNO50 = column_names[49]\n",
    "\n",
    "    PARTNO51 = column_names[50]\n",
    "    PARTNO52 = column_names[51]\n",
    "    PARTNO53 = column_names[52]\n",
    "    PARTNO54 = column_names[53]\n",
    "    PARTNO55 = column_names[54]\n",
    "    PARTNO56 = column_names[55]\n",
    "\n",
    "    # Prediction data standardize\n",
    "    # Common variables\n",
    "    PARTNO01N = norm(PARTNO01)\n",
    "    PARTNO02N = norm(PARTNO02)\n",
    "    PARTNO03N = norm(PARTNO03)\n",
    "    PARTNO04N = norm(PARTNO04) \n",
    "    PARTNO05N = norm(PARTNO05)\n",
    "    PARTNO06N = norm(PARTNO06)\n",
    "    PARTNO07N = norm(PARTNO07)\n",
    "    PARTNO08N = norm(PARTNO08)\n",
    "    PARTNO09N = norm(PARTNO09) \n",
    "    PARTNO10N = norm(PARTNO10)\n",
    "\n",
    "    PARTNO11N = norm(PARTNO11)\n",
    "    PARTNO12N = norm(PARTNO12)\n",
    "    PARTNO13N = norm(PARTNO13)\n",
    "    PARTNO14N = norm(PARTNO14) \n",
    "    PARTNO15N = norm(PARTNO15)\n",
    "    PARTNO16N = norm(PARTNO16)\n",
    "    PARTNO17N = norm(PARTNO17)\n",
    "    PARTNO18N = norm(PARTNO18)\n",
    "    PARTNO19N = norm(PARTNO19) \n",
    "    PARTNO20N = norm(PARTNO20)\n",
    "\n",
    "    PARTNO21N = norm(PARTNO21)\n",
    "    PARTNO22N = norm(PARTNO22)\n",
    "    PARTNO23N = norm(PARTNO23)\n",
    "    PARTNO24N = norm(PARTNO24) \n",
    "    PARTNO25N = norm(PARTNO25)\n",
    "    PARTNO26N = norm(PARTNO26)\n",
    "    PARTNO27N = norm(PARTNO27)\n",
    "    PARTNO28N = norm(PARTNO28)\n",
    "    PARTNO29N = norm(PARTNO29) \n",
    "    PARTNO30N = norm(PARTNO30)\n",
    "\n",
    "    PARTNO31N = norm(PARTNO31)\n",
    "    PARTNO32N = norm(PARTNO32)\n",
    "    PARTNO33N = norm(PARTNO33)\n",
    "    PARTNO34N = norm(PARTNO34) \n",
    "    PARTNO35N = norm(PARTNO35)\n",
    "    PARTNO36N = norm(PARTNO36)\n",
    "    PARTNO37N = norm(PARTNO37)\n",
    "    PARTNO38N = norm(PARTNO38)\n",
    "    PARTNO39N = norm(PARTNO39) \n",
    "    PARTNO40N = norm(PARTNO40)\n",
    "\n",
    "    PARTNO41N = norm(PARTNO41)\n",
    "    PARTNO42N = norm(PARTNO42)\n",
    "    PARTNO43N = norm(PARTNO43)\n",
    "    PARTNO44N = norm(PARTNO44) \n",
    "    PARTNO45N = norm(PARTNO45)\n",
    "    PARTNO46N = norm(PARTNO46)\n",
    "\n",
    "    PARTNO47N = norm(PARTNO47)\n",
    "    PARTNO48N = norm(PARTNO48)\n",
    "    PARTNO49N = norm(PARTNO49)\n",
    "    PARTNO50N = norm(PARTNO50) \n",
    "    PARTNO51N = norm(PARTNO51)\n",
    "    PARTNO52N = norm(PARTNO52)\n",
    "    PARTNO53N = norm(PARTNO53)\n",
    "    PARTNO54N = norm(PARTNO54)\n",
    "    PARTNO55N = norm(PARTNO55) \n",
    "    PARTNO56N = norm(PARTNO56)    \n",
    "\n",
    "    # Result of prediction data   \n",
    "    predict_cur_Area = model_Area.predict([[\n",
    "        PARTNO01N,        PARTNO02N,        PARTNO03N,        PARTNO04N,        PARTNO05N, \\\n",
    "        PARTNO06N,        PARTNO07N,        PARTNO08N,        PARTNO09N,        PARTNO10N, \\\n",
    "        PARTNO11N,        PARTNO12N,        PARTNO13N,        PARTNO14N,        PARTNO15N, \\\n",
    "        PARTNO16N,        PARTNO17N,        PARTNO18N,        PARTNO19N,        PARTNO20N, \\\n",
    "        PARTNO21N,        PARTNO22N,        PARTNO23N,        PARTNO24N,        PARTNO25N, \\\n",
    "        PARTNO26N,        PARTNO27N,        PARTNO28N,        PARTNO29N,        PARTNO30N, \\\n",
    "        PARTNO31N,        PARTNO32N,        PARTNO33N,        PARTNO34N,        PARTNO35N, \\\n",
    "        PARTNO36N,        PARTNO37N,        PARTNO38N,        PARTNO39N,        PARTNO40N, \\\n",
    "        PARTNO41N,        PARTNO42N,        PARTNO43N,        PARTNO44N,        PARTNO45N, \\\n",
    "        PARTNO46N,        PARTNO47N,        PARTNO48N,        PARTNO49N,        PARTNO50N, \\\n",
    "        PARTNO51N,        PARTNO52N,        PARTNO53N,        PARTNO54N,        PARTNO55N, \\\n",
    "        PARTNO56N\n",
    "        ]])[0]\n",
    "    predict_cur_Pelvic = model_Pelvic.predict([[\n",
    "        PARTNO01N,        PARTNO02N,        PARTNO03N,        PARTNO04N,        PARTNO05N, \\\n",
    "        PARTNO06N,        PARTNO07N,        PARTNO08N,        PARTNO09N,        PARTNO10N, \\\n",
    "        PARTNO11N,        PARTNO12N,        PARTNO13N,        PARTNO14N,        PARTNO15N, \\\n",
    "        PARTNO16N,        PARTNO17N,        PARTNO18N,        PARTNO19N,        PARTNO20N, \\\n",
    "        PARTNO21N,        PARTNO22N,        PARTNO23N,        PARTNO24N,        PARTNO25N, \\\n",
    "        PARTNO26N,        PARTNO27N,        PARTNO28N,        PARTNO29N,        PARTNO30N, \\\n",
    "        PARTNO31N,        PARTNO32N,        PARTNO33N,        PARTNO34N,        PARTNO35N, \\\n",
    "        PARTNO36N,        PARTNO37N,        PARTNO38N,        PARTNO39N,        PARTNO40N, \\\n",
    "        PARTNO41N,        PARTNO42N,        PARTNO43N,        PARTNO44N,        PARTNO45N, \\\n",
    "        PARTNO46N,        PARTNO47N,        PARTNO48N,        PARTNO49N,        PARTNO50N, \\\n",
    "        PARTNO51N,        PARTNO52N,        PARTNO53N,        PARTNO54N,        PARTNO55N, \\\n",
    "        PARTNO56N\n",
    "        ]])[0]        \n",
    " \n",
    "\n",
    "    norm_Area   = (predict_cur_Area   - min_Area  )/(max_Area   - min_Area  )\n",
    "    norm_Pelvic = (predict_cur_Pelvic - min_Pelvic)/(max_Pelvic - min_Pelvic)\n",
    "    print(predict_cur_Area, predict_cur_Pelvic)\n",
    "    # print(predict_cur_Pelvic)\n",
    "    # print(norm_Area + norm_Pelvic)\n",
    "\n",
    "\n",
    "    return   norm_Area + norm_Pelvic \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bnds; doundary conditions\n",
    "# Parameter of Boundary conditions\n",
    "bnds = [( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), \n",
    "        ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), \n",
    "        ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), \n",
    "        ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), \n",
    "        ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), \n",
    "        ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), \n",
    "        ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), \n",
    "        ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ),                       \n",
    "        ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), \n",
    "        ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ),\n",
    "        ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ), ( 440, 1400 ),\n",
    "        ( 440, 1400 )\n",
    "]\n",
    "print(len(bnds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "# best1bin\n",
    "result_best1bin = optimize.differential_evolution(predict_func, bnds, strategy='best1bin', init='latinhypercube')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get minimize parameters\n",
    "minimum_best1bin_x = result_best1bin.x\n",
    "minimum_best1bin_f = result_best1bin.fun\n",
    "minimum_best1bin_x, minimum_best1bin_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range( 0, len(minimum_best1bin_x) ):\n",
    "    if minimum_best1bin_x[i] <= 515 :\n",
    "        minimum_best1bin_x[i] = 440\n",
    "    elif 515 < minimum_best1bin_x[i] <= 685:\n",
    "        minimum_best1bin_x[i] = 590\n",
    "    elif 685 < minimum_best1bin_x[i] <= 880:\n",
    "        minimum_best1bin_x[i] = 780\n",
    "    elif 880 < minimum_best1bin_x[i] <= 1080:\n",
    "        minimum_best1bin_x[i] = 980\n",
    "    elif 1080 < minimum_best1bin_x[i] <= 1290:\n",
    "        minimum_best1bin_x[i] = 1180\n",
    "    elif 1290 < minimum_best1bin_x[i] :\n",
    "        minimum_best1bin_x[i] = 1400\n",
    "minimum_best1bin_x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range( 0, len(minimum_best1bin_x) ):\n",
    "    if minimum_best1bin_x[i] <= 515 : \n",
    "        minimum_best1bin_x[i] = 11380000\n",
    "    elif 515 < minimum_best1bin_x[i] <= 685:\n",
    "        minimum_best1bin_x[i] = 11130000\n",
    "    elif 685 < minimum_best1bin_x[i] <= 880:\n",
    "        minimum_best1bin_x[i] = 11160000\n",
    "    elif 880 < minimum_best1bin_x[i] <= 1080:\n",
    "        minimum_best1bin_x[i] = 11190000\n",
    "    elif 1080 < minimum_best1bin_x[i] <= 1290:\n",
    "        minimum_best1bin_x[i] = 11210000\n",
    "    elif 1290 < minimum_best1bin_x[i] :\n",
    "        minimum_best1bin_x[i] = 60101030\n",
    "minimum_best1bin_x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for val in minimum_best1bin_x:\n",
    "    print(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import least_squares\n",
    "n = 56\n",
    "def jac(n):\n",
    "    i = np.arange(n)\n",
    "    jj, ii = np.meshgrid(i,i)\n",
    "    \n",
    "u = np.array([440,590,780,980,1180,1400])\n",
    "res = least_squares(predict_func, u,  verbose=1)"
   ]
  }
 ]
}